{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsgHXrZjF1U_"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import KFold\n",
        "from google.colab import drive\n",
        "\n",
        "# =============================================================================\n",
        "# 0. Global Environment & Resource Management\n",
        "# =============================================================================\n",
        "print(\"ðŸ“¦ Initializing Environment...\")\n",
        "\n",
        "# Explicit memory cleanup to prevent OOM on Colab high-RAM instances\n",
        "try:\n",
        "    del model, tokenizer, optimizer, scheduler\n",
        "except NameError:\n",
        "    pass\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Mount Google Drive for persistent storage of model checkpoints\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Hyperparameters & Hardware Optimization (A100 Optimized)\n",
        "# =============================================================================\n",
        "CONFIG = {\n",
        "    \"model_name\": \"roberta-large\", # Base model for fine-tuning\n",
        "\n",
        "    # Dataset Paths\n",
        "    \"train_files\": [\n",
        "        os.path.join(BASE_PATH, \"dev_track_a.jsonl\"),\n",
        "        os.path.join(BASE_PATH, \"synthetic_data_for_contrastive_learning.jsonl\")\n",
        "    ],\n",
        "    \"test_file\":  os.path.join(BASE_PATH, \"test_track_a.jsonl\"),\n",
        "\n",
        "    # Output Configuration\n",
        "    \"output_file\": os.path.join(BASE_PATH, \"track_a_submission_roberta_5fold_fast_epoch10.jsonl\"),\n",
        "\n",
        "    # Training Parameters\n",
        "    \"max_len\": 512,\n",
        "    \"batch_size\": 32,      # High throughput for A100 GPU\n",
        "    \"accum_steps\": 1,      # Gradient accumulation steps (1 for direct optimization)\n",
        "    \"epochs\": 10,\n",
        "    \"lr\": 1e-5,\n",
        "    \"n_folds\": 5,          # Cross-validation folds\n",
        "    \"seed\": 42,\n",
        "    \"device\": \"cuda\",\n",
        "    \"num_workers\": 4       # Multi-process data loading\n",
        "}\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"Ensure reproducibility across experiments.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(CONFIG['seed'])\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Data Engineering & Dataset Definition\n",
        "# =============================================================================\n",
        "def load_all_data(file_paths):\n",
        "    \"\"\"Load and parse JSONL files, supporting both ranking and contrastive formats.\"\"\"\n",
        "    all_data = []\n",
        "    print(\"\\n[Step 1] Loading Datasets...\")\n",
        "    for file_path in file_paths:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"âš ï¸ Warning: {file_path} not found. Skipping.\")\n",
        "            continue\n",
        "        print(f\"ðŸ“– Reading: {os.path.basename(file_path)}\")\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    item = json.loads(line)\n",
        "                    # Standard ranking format\n",
        "                    if item.get('anchor_text'):\n",
        "                        all_data.append(item)\n",
        "                    # Contrastive learning triplet format (Anchor, Pos, Neg)\n",
        "                    elif item.get('anchor_story'):\n",
        "                        anchor, pos, neg = item['anchor_story'], item['similar_story'], item['dissimilar_story']\n",
        "                        # Randomly flip to prevent label bias\n",
        "                        if random.random() > 0.5:\n",
        "                            all_data.append({\"anchor_text\": anchor, \"text_a\": pos, \"text_b\": neg, \"text_a_is_closer\": True})\n",
        "                        else:\n",
        "                            all_data.append({\"anchor_text\": anchor, \"text_a\": neg, \"text_b\": pos, \"text_a_is_closer\": False})\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "    print(f\"âœ… Total Samples Loaded: {len(all_data)}\")\n",
        "    return np.array(all_data)\n",
        "\n",
        "class RankingDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for Pairwise Ranking tasks.\"\"\"\n",
        "    def __init__(self, data_list, tokenizer, max_len, is_test=False):\n",
        "        self.data = data_list\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        anchor = item['anchor_text']\n",
        "\n",
        "        # Tokenize both pairs: (Anchor, Text_A) and (Anchor, Text_B)\n",
        "        pair_a = self.tokenizer(anchor, item['text_a'], truncation=True, max_length=self.max_len, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        pair_b = self.tokenizer(anchor, item['text_b'], truncation=True, max_length=self.max_len, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        inputs = {\n",
        "            'input_ids_a': pair_a['input_ids'].squeeze(0),\n",
        "            'mask_a': pair_a['attention_mask'].squeeze(0),\n",
        "            'input_ids_b': pair_b['input_ids'].squeeze(0),\n",
        "            'mask_b': pair_b['attention_mask'].squeeze(0),\n",
        "        }\n",
        "\n",
        "        if not self.is_test:\n",
        "            # MarginRankingLoss Target: 1.0 if A is closer than B, else -1.0\n",
        "            label = 1.0 if item.get('text_a_is_closer') else -1.0\n",
        "            inputs['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Training Pipeline (with Automated Mixed Precision - AMP)\n",
        "# =============================================================================\n",
        "def train_one_fold(fold_idx, train_data, val_data):\n",
        "    \"\"\"Executes the training loop for a single cross-validation fold.\"\"\"\n",
        "    print(f\"\\nðŸš€ Commencing Fold {fold_idx+1}/{CONFIG['n_folds']}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(CONFIG['model_name'], num_labels=1).to(CONFIG['device'])\n",
        "\n",
        "    train_ds = RankingDataset(train_data, tokenizer, CONFIG['max_len'])\n",
        "    val_ds = RankingDataset(val_data, tokenizer, CONFIG['max_len'])\n",
        "\n",
        "    # Optimized DataLoaders with pin_memory for faster host-to-device transfer\n",
        "    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], num_workers=CONFIG['num_workers'], pin_memory=True)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=0.01)\n",
        "    criterion = torch.nn.MarginRankingLoss(margin=0.5)\n",
        "\n",
        "    # Initialize Gradient Scaler for AMP\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    best_val_acc = 0\n",
        "    save_path = os.path.join(BASE_PATH, f\"roberta_fold_{fold_idx}.pt\")\n",
        "\n",
        "    for epoch in range(CONFIG['epochs']):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        progress = tqdm(train_loader, desc=f\"Fold {fold_idx+1} Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "        for batch in progress:\n",
        "            ids_a, mask_a = batch['input_ids_a'].to(CONFIG['device']), batch['mask_a'].to(CONFIG['device'])\n",
        "            ids_b, mask_b = batch['input_ids_b'].to(CONFIG['device']), batch['mask_b'].to(CONFIG['device'])\n",
        "            target = batch['labels'].to(CONFIG['device'])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with Autocast for FP16 precision\n",
        "            with torch.cuda.amp.autocast():\n",
        "                out_a = model(input_ids=ids_a, attention_mask=mask_a).logits.squeeze(-1)\n",
        "                out_b = model(input_ids=ids_b, attention_mask=mask_b).logits.squeeze(-1)\n",
        "                loss = criterion(out_a, out_b, target)\n",
        "\n",
        "            # Backward pass with Scaled Gradients\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                ids_a, mask_a = batch['input_ids_a'].to(CONFIG['device']), batch['mask_a'].to(CONFIG['device'])\n",
        "                ids_b, mask_b = batch['input_ids_b'].to(CONFIG['device']), batch['mask_b'].to(CONFIG['device'])\n",
        "                target = batch['labels'].to(CONFIG['device'])\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    s_a = model(input_ids=ids_a, attention_mask=mask_a).logits.squeeze(-1)\n",
        "                    s_b = model(input_ids=ids_b, attention_mask=mask_b).logits.squeeze(-1)\n",
        "\n",
        "                pred_a_better = s_a > s_b\n",
        "                true_a_better = target > 0\n",
        "                correct += (pred_a_better == true_a_better).sum().item()\n",
        "                total += len(target)\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"   [Fold {fold_idx+1} | Epoch {epoch+1}] Validation Accuracy: {acc:.4f}\")\n",
        "\n",
        "        # Model Checkpointing\n",
        "        if acc > best_val_acc:\n",
        "            best_val_acc = acc\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"   ðŸŒŸ New Best Accuracy. Model saved to {os.path.basename(save_path)}\")\n",
        "\n",
        "    # Memory cleanup after fold completion\n",
        "    del model, optimizer, scaler, train_loader\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Inference & Model Ensembling\n",
        "# =============================================================================\n",
        "def inference_ensemble():\n",
        "    \"\"\"Performs inference using a 5-fold ensemble approach for maximum robustness.\"\"\"\n",
        "    print(\"\\n[Step 3] Ensemble Inference (5-Fold Strategy)...\")\n",
        "\n",
        "    test_data_raw = []\n",
        "    with open(CONFIG['test_file'], 'r') as f:\n",
        "        for line in f:\n",
        "            test_data_raw.append(json.loads(line))\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "    test_ds = RankingDataset(test_data_raw, tokenizer, CONFIG['max_len'], is_test=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=64, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
        "\n",
        "    all_fold_scores = np.zeros((len(test_data_raw), CONFIG['n_folds']))\n",
        "\n",
        "    for fold in range(CONFIG['n_folds']):\n",
        "        model_path = os.path.join(BASE_PATH, f\"roberta_fold_{fold}.pt\")\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"âš ï¸ Fold {fold} checkpoint not found. Skipping inference.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"ðŸ¤– Inferencing with Fold {fold} weights...\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(CONFIG['model_name'], num_labels=1).to(CONFIG['device'])\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        model.eval()\n",
        "\n",
        "        fold_preds = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=f\"Predicting Fold {fold}\"):\n",
        "                ids_a, mask_a = batch['input_ids_a'].to(CONFIG['device']), batch['mask_a'].to(CONFIG['device'])\n",
        "                ids_b, mask_b = batch['input_ids_b'].to(CONFIG['device']), batch['mask_b'].to(CONFIG['device'])\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    s_a = model(input_ids=ids_a, attention_mask=mask_a).logits.squeeze(-1)\n",
        "                    s_b = model(input_ids=ids_b, attention_mask=mask_b).logits.squeeze(-1)\n",
        "                    diff = s_a - s_b # Relative score difference\n",
        "                fold_preds.extend(diff.cpu().numpy())\n",
        "\n",
        "        all_fold_scores[:, fold] = fold_preds\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Averaging scores across all folds (Soft Voting/Ensemble)\n",
        "    avg_scores = np.mean(all_fold_scores, axis=1)\n",
        "    final_results = [{\"text_a_is_closer\": bool(score > 0)} for score in avg_scores]\n",
        "\n",
        "    # Writing results to JSONL submission format\n",
        "    with open(CONFIG['output_file'], 'w') as f:\n",
        "        for res in final_results:\n",
        "            f.write(json.dumps(res) + \"\\n\")\n",
        "\n",
        "    print(f\"\\nâœ… Pipeline Complete! Submission file generated at: {CONFIG['output_file']}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 5. Main Execution Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and shuffle training data\n",
        "    all_data = load_all_data(CONFIG['train_files'])\n",
        "\n",
        "    # Define K-Fold Cross-Validation strategy\n",
        "    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
        "\n",
        "    print(f\"ðŸ”„ Starting {CONFIG['n_folds']}-Fold Cross Validation Workflow...\")\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_data)):\n",
        "        train_fold = all_data[train_idx]\n",
        "        val_fold = all_data[val_idx]\n",
        "        train_one_fold(fold, train_fold, val_fold)\n",
        "\n",
        "    # Run ensemble inference on test set\n",
        "    inference_ensemble()"
      ]
    }
  ]
}